{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models.detection import SSD300_VGG16_Weights\n",
    "from torchvision.models.vgg import VGG16_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.models.detection import ssd\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import PIL.Image\n",
    "import torchvision.transforms.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\Domi\\Documents\\GitHub\\Deep-Vision-sta\\Datasets\\Face Mask Detection Dataset\\Medical mask\\Medical mask\\Medical Mask\"\n",
    "\n",
    "NORMALIZE = True\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "RESIZE = (320, 320)\n",
    "ROUND_RESIZED_BBOXES = False\n",
    "LEARNING_RATE = 0.005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "NESTEROV = True\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "ALLOWED_LABELS = [3, 4, 5, 6]\n",
    "class_mapping = {\n",
    "    \"hijab_niqab\": 0,\n",
    "    \"mask_colorful\": 1,\n",
    "    \"mask_surgical\": 2,\n",
    "    \"face_no_mask\": 3,\n",
    "    \"face_with_mask_incorrect\": 4,\n",
    "    \"face_with_mask\": 5,\n",
    "    \"face_other_covering\": 6,\n",
    "    \"scarf_bandana\": 7,\n",
    "    \"balaclava_ski_mask\": 8,\n",
    "    \"face_shield\": 9,\n",
    "    \"other\": 10,\n",
    "    \"gas_mask\": 11,\n",
    "    \"turban\": 12,\n",
    "    \"helmet\": 13,\n",
    "    \"sunglasses\": 14,\n",
    "    \"eyeglasses\": 15,\n",
    "    \"hair_net\": 16,\n",
    "    \"hat\": 17,\n",
    "    \"goggles\": 18,\n",
    "    \"hood\": 19\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATACLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.images_dir = os.path.join(root_dir, \"images\")\n",
    "        self.annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "    \n",
    "    \n",
    "    def load_dataset(self):\n",
    "        dataset = []\n",
    "        supported_image_extensions = (\".jpg\", \".jpeg\", \".png\")\n",
    "        supported_annotation_extensions = (\".jpg.json\", \".jpeg.json\", \".png.json\")  # Beispielerweiterungen, anpassen Sie dies entsprechend\n",
    "\n",
    "        for filename in os.listdir(self.images_dir):\n",
    "            if filename.lower().endswith(supported_image_extensions):\n",
    "                image_path = os.path.join(self.images_dir, filename)\n",
    "\n",
    "                # Annotationen\n",
    "                annotation_filename = os.path.splitext(filename)[0]\n",
    "                for extension in supported_annotation_extensions:\n",
    "                    annotation_file = annotation_filename + extension\n",
    "                    annotation_path = os.path.join(self.annotations_dir, annotation_file)\n",
    "                    if os.path.exists(annotation_path):\n",
    "                        break\n",
    "\n",
    "                # Bild und Annotationen einlesen\n",
    "                image, annotations = self._read_data(image_path, annotation_path)\n",
    "                dataset.append((image, annotations))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "\n",
    "\n",
    "    def _read_data(self, image_path, annotation_path):\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        with open(annotation_path, 'r') as f:\n",
    "            annotations = json.load(f)\n",
    "        \n",
    "        image_annotations = {\n",
    "            \"filename\": annotations[\"FileName\"],\n",
    "            \"annotations\": []\n",
    "        }\n",
    "        \n",
    "        for annotation in annotations[\"Annotations\"]:\n",
    "            bbox = annotation[\"BoundingBox\"]\n",
    "            xmin, ymin, xmax, ymax = bbox\n",
    "            label = annotation[\"classname\"]\n",
    "            \n",
    "            formatted_annotation = {\n",
    "                \"bbox\": [xmin, ymin, xmax, ymax],\n",
    "                \"label\": label\n",
    "            }\n",
    "            \n",
    "            image_annotations[\"annotations\"].append(formatted_annotation)\n",
    "        \n",
    "        return image, image_annotations\n",
    "    \n",
    "\n",
    "# class MyCustomDataset(Dataset):\n",
    "#     def __init__(self, dataset):\n",
    "#         self.dataset = dataset\n",
    "#         self.transform = ToTensor()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         data = self.dataset[index]\n",
    "#         image = self.transform(data[0])  # Bild in Tensor umwandeln\n",
    "#         annotations = data[1]  # Annotationen beibehalten\n",
    "\n",
    "#         return image, annotations   \n",
    "    \n",
    "def check_labels(target):\n",
    "    labels = target['labels']\n",
    "    boxes = target['boxes']\n",
    "\n",
    "    num_boxes = boxes.size(0)\n",
    "    num_labels = labels.size(0)\n",
    "\n",
    "    if num_labels != num_boxes:\n",
    "        print(\"Error: Labels are not in the expected format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_size=(600, 900)):\n",
    "        self.root_dir = root_dir\n",
    "        self.annotations = []\n",
    "        self.target_size = target_size\n",
    "        self.load_annotations()\n",
    "\n",
    "    def load_annotations(self):\n",
    "        annotation_files = os.listdir(f\"{self.root_dir}/annotations\")\n",
    "        for file_name in annotation_files:\n",
    "            with open(f\"{self.root_dir}/annotations/{file_name}\", \"r\") as f:\n",
    "                annotation_data = json.load(f)\n",
    "                annotations = annotation_data[\"Annotations\"]\n",
    "                file_name = annotation_data[\"FileName\"]\n",
    "                self.annotations.append((annotations, file_name))\n",
    "                # Check if the boxes are valid\n",
    "                for annotation in annotations:\n",
    "                    boxes = annotation[\"BoundingBox\"]\n",
    "                    if boxes[0] >= boxes[2] or boxes[1] >= boxes[3]:\n",
    "                        print(\"Invalid bounding box coordinates in file:\", file_name)\n",
    "                        break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotations = self.annotations[idx][0]\n",
    "        file_name = self.annotations[idx][1]\n",
    "        image_path = f\"{self.root_dir}/images/{file_name}\"\n",
    "        image = PIL.Image.open(image_path).convert(\"RGB\")\n",
    "        original_image_width, original_image_height = image.size\n",
    "        image = F.resize(image, self.target_size)\n",
    "        image = F.to_tensor(image)\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for annotation in annotations:\n",
    "            box = annotation[\"BoundingBox\"]\n",
    "            if box[0] < box[2] and box[1] < box[3]:\n",
    "                # Resize the bounding box coordinates\n",
    "                box_resized = [\n",
    "                    box[0] * self.target_size[0] / original_image_width,\n",
    "                    box[1] * self.target_size[1] / original_image_height,\n",
    "                    box[2] * self.target_size[0] / original_image_width,\n",
    "                    box[3] * self.target_size[1] / original_image_height\n",
    "                ]\n",
    "                boxes.append(box_resized)\n",
    "                class_name = annotation[\"classname\"]\n",
    "                # Get the class label based on the class name\n",
    "                class_label = self.get_class_label(class_name)\n",
    "                labels.append(class_label)\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    \n",
    "    def get_class_label(self, class_name):\n",
    "        return class_mapping.get(class_name, -1)  # Return -1 if class_name is not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = DatasetLoader(root_dir)\n",
    "dataset = dataset_loader.load_dataset()\n",
    "image, annotations = dataset[0]\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "# def collate_fn(batch):\n",
    "#     images = []\n",
    "#     annotations = []    \n",
    "\n",
    "#     # Definiere die Zielgröße für das Rescaling\n",
    "#     target_size = RESIZE\n",
    "\n",
    "#     # Erstelle die Rescaling-Transformation\n",
    "#     rescale_transform = transforms.Resize(target_size, interpolation=Image.Resampling.BILINEAR)\n",
    "\n",
    "#     for image, annotation in batch:\n",
    "#         # Wandele den Tensor in eine PIL-Image-Instanz um\n",
    "#         image_size = image.size()\n",
    "#         image = transforms.ToPILImage()(image)\n",
    "\n",
    "#         # Wende die Rescaling-Transformation auf das Bild an\n",
    "#         image = rescale_transform(image)\n",
    "\n",
    "#         # Konvertiere das Bild in ein Tensor und füge es zur Liste hinzu\n",
    "#         image = transforms.ToTensor()(image)\n",
    "#         images.append(image)\n",
    "\n",
    "#         # Passe die Bounding-Boxen an die neue Größe des Bildes an\n",
    "#         width_ratio = target_size[0] / image_size[2]\n",
    "#         height_ratio = target_size[1] / image_size[1]\n",
    "#         for bbox_dict in annotation['annotations']:\n",
    "#             bbox = bbox_dict['bbox']\n",
    "#             x_min, y_min, x_max, y_max = bbox\n",
    "#             x_min *= width_ratio\n",
    "#             y_min *= height_ratio\n",
    "#             x_max *= width_ratio\n",
    "#             y_max *= height_ratio\n",
    "#             bbox_dict['bbox'] = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "#         # Füge die Annotations zur Annotations-Liste hinzu\n",
    "#         annotations.append(annotation)\n",
    "\n",
    "#     # Passe die Größen der Bilder an, um stapelbar zu sein\n",
    "#     images = torch.stack(images)\n",
    "\n",
    "#     # Normalize die Pixelwerte der Bilder\n",
    "#     if NORMALIZE:\n",
    "#         images = transforms.Normalize(mean=MEAN, std=STD)(images)\n",
    "\n",
    "#     return images, annotations\n",
    "\n",
    "from torchvision.models.detection import SSDLite320_MobileNet_V3_Large_Weights\n",
    "from torchvision.models import MobileNet_V3_Large_Weights\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "\n",
    "\n",
    "def setup_model(batch_size, dataset, lr, momentum, weight_decay, nesterov, test_size, weights_backbone=None, weights=None,):\n",
    "    # Modell initialisieren\n",
    "    model = ssdlite320_mobilenet_v3_large(weights=weights, weights_backbone=weights_backbone)\n",
    "\n",
    "    # Daten in Trainings- und Testdaten aufteilen\n",
    "    #train_data, test_data = train_test_split(dataset, test_size=TEST_SIZE, random_state=42)\n",
    "    dataset = MaskDetectionDataset(root_dir, RESIZE)\n",
    "    train_size = int((1-test_size) * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Trainingsdaten vorbereiten und DataLoader erstellen\n",
    "    #train_dataset = MyCustomDataset(train_data)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "    # Testdaten vorbereiten und DataLoader erstellen\n",
    "    #test_dataset = MaskDetectionDataset(root_dir, RESIZE)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.00005, momentum=0.9, weight_decay=0.0005, nesterov=True)    \n",
    "\n",
    "    return model, train_dataloader, test_dataloader, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(dataloader, x):\n",
    "    # Rufen Sie das x-te Element aus dem Dataloader ab\n",
    "    x -=1 \n",
    "    images, annotations = next(iter(dataloader))\n",
    "    image = transforms.ToPILImage(images[x])\n",
    "    boxes = annotations[x]['annotations']\n",
    "    labels = [box['label'] for box in boxes]\n",
    "    print(labels)\n",
    "    # Erstellen Sie eine neue Figur und Achse\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Zeigen Sie das Bild in der Achse an\n",
    "    ax.imshow(image.permute(1, 2, 0))\n",
    "    # Iterieren Sie über die Bounding Boxes und zeichnen Sie sie als Rechtecke in der Achse\n",
    "    for box, label in (boxes, labels):\n",
    "        x_min, y_min, x_max, y_max = box['bbox']\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        #print(width, height)\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        #ax.text(x_min, y_min, f\"Label: {class_mapping[label]}\", color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "        ax.text(x_min, y_min, f\"{label}\", color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    # Zeigen Sie die visualisierten Bounding Boxes an\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_image_with_boxes(image, target):    \n",
    "    # Unnormalize the image\n",
    "    if NORMALIZE:\n",
    "        image = transforms.Normalize(mean=[-m / s for m, s in zip(MEAN, STD)], std=[1 / s for s in STD])(image)\n",
    "    image_pil = transforms.ToPILImage()(image)\n",
    "\n",
    "    # Kopiere die Bounding-Box-Koordinaten auf die CPU und konvertiere sie in numpy-Arrays\n",
    "    boxes = target[\"boxes\"]\n",
    "    labels = target[\"labels\"]\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "        \n",
    "    # Erstelle eine neue Figur und Achse\n",
    "    fig, ax = plt.subplots(1)    \n",
    "    # Zeige das Bild in der Achse\n",
    "    ax.imshow(image_pil)\n",
    "    print(target)\n",
    "    \n",
    "    # Iteriere über die Bounding-Boxen und zeichne sie als Rechtecke in der Achse\n",
    "    for box, label in zip(boxes, labels):\n",
    "        for label in ALLOWED_LABELS:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "            rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x_min, y_min, f\"Label: {class_mapping[label]}\", color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "    # Zeige die Achse\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_prediction(images, model, confidence_threshold, counter = 10):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval() \n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "        #good = torch.argwhere(scores > confidence_threshold)\n",
    "\n",
    "    for image, prediction in zip(images, predictions):\n",
    "        if NORMALIZE:\n",
    "            # Unnormalize the image\n",
    "            image = transforms.Normalize(mean=[-m / s for m, s in zip(MEAN, STD)], std=[1 / s for s in STD])(image)\n",
    "        image_pil = transforms.ToPILImage()(image)\n",
    "\n",
    "        # Get the predicted bounding boxes, labels, and scores\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        labels = prediction['labels'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "        # Visualize the image and predicted bounding boxes\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(image_pil)\n",
    "        allowed_labels = [3, 4, 5, 6]\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if label in allowed_labels and score > confidence_threshold and counter%10 == 0:\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                class_name = list(class_mapping.keys())[list(class_mapping.values()).index(label)]\n",
    "                rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x_min, y_min, f\"{class_name}: {score}\", color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "                counter=0\n",
    "        plt.show()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize_sample(train_dataloader, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from coco_eval import CocoEvaluator\n",
    "\n",
    "\n",
    "def calculate_mAP(model, dataloader, device):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    coco_evaluator = CocoEvaluator(dataloader, iou_types=\"bbox\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images, targets)\n",
    "            coco_evaluator.update(targets, outputs)\n",
    "\n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "\n",
    "    mAP = coco_evaluator.stats[0]\n",
    "\n",
    "    return mAP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_own(model, optimizer, train_dataloader, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    total_iterations = len(train_dataloader)\n",
    "    total_loss = 0.0\n",
    "    counter = 0\n",
    "    \n",
    "    for images, annotations in pbar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Annotationsdaten aufbereiten\n",
    "        targets = []\n",
    "\n",
    "        for annotation in annotations:\n",
    "            boxes = annotation[\"annotations\"]\n",
    "            labels = [box[\"label\"] for box in boxes]\n",
    "            bboxes = [box[\"bbox\"] for box in boxes]\n",
    "            #print(boxes[0])\n",
    "\n",
    "            # Wandele Labels in numerische Werte um\n",
    "            labels = [class_mapping[label] for label in labels]\n",
    "\n",
    "            target = {\n",
    "                \"boxes\": torch.tensor(bboxes, dtype=torch.float32).to(device),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "            }\n",
    "            check_labels(target)\n",
    "            targets.append(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        # print(targets[0]['boxes'].size())\n",
    "        # print(images[0].shape)\n",
    "        # print(images[0])\n",
    "        # print(targets[0])\n",
    "        # break\n",
    "\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        average_loss = total_loss / (pbar.n + 1)\n",
    "        \n",
    "        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Average-Loss: {average_loss:.4f}, Loss: {losses.item():.4f}\")\n",
    "\n",
    "        #draw_image_with_boxes(images[0], targets[0])\n",
    "        if (counter + 1) % (total_iterations // 20) == 0:\n",
    "            visualize_prediction([images[0]], model, confidence_threshold=0.3)\n",
    "        #visualize_prediction(images, model, confidence_threshold=0.3, counter=counter)\n",
    "        counter += 1\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_dataloader, test_dataloader, optimizer = setup_model(BATCH_SIZE, \n",
    "                                                       weights_backbone= MobileNet_V3_Large_Weights.DEFAULT, \n",
    "                                                       weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT, \n",
    "                                                       dataset=dataset,\n",
    "                                                       lr=LEARNING_RATE,\n",
    "                                                       momentum=MOMENTUM,\n",
    "                                                       weight_decay=WEIGHT_DECAY,\n",
    "                                                       nesterov=NESTEROV,\n",
    "                                                       test_size=TEST_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_one_epoch_own(model, optimizer, train_dataloader, device, epoch, num_epochs)\n",
    "#     #mAP = calculate_mAP(model, test_dataloader, device)\n",
    "#     #print(f\"Epoch [{epoch+1}/{num_epochs}], mAP: {mAP:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(test_dataloader))\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "\n",
    "# # Trainingsschleife\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define empty arrays to collect metrics\n",
    "ap_values = []\n",
    "ar_values = []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training for one epoch\n",
    "    train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=1, losses_out=losses)\n",
    "    # update the learning rate\n",
    "    # lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    # adjusted numpy library code for evaluate to work with newer numpy versions\n",
    "    evaluator = evaluate(model, test_dataloader, device=device)\n",
    "    #print(evaluator)\n",
    "\n",
    "    # Extract the metrics from the evaluator\n",
    "    iou_thresholds = evaluator.coco_eval['bbox'].params.iouThrs\n",
    "    average_precisions = evaluator.coco_eval['bbox'].stats[:6]\n",
    "    average_recalls = evaluator.coco_eval['bbox'].stats[6:]\n",
    "\n",
    "    # Append the metrics to the arrays\n",
    "    ap_values.append(average_precisions)\n",
    "    ar_values.append(average_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Convert the arrays to numpy arrays for easier plotting\n",
    "ap_values = np.array(ap_values)\n",
    "ar_values = np.array(ar_values)\n",
    "\n",
    "iou_thresholds_available = [\"0.50:0.95\", \"0.50\", \"0.75\", \"0.50:0.95_small\", \"0.50:0.95_medium\", \"0.50:0.95_large\"]\n",
    "\n",
    "# Plot the average precisions over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, iou_thresh in enumerate(iou_thresholds_available):\n",
    "    plt.plot(ap_values[:, i], label=f\"IoU={iou_thresh}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Precision\")\n",
    "plt.title(\"Average Precision vs. Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the average recalls over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, iou_thresh in enumerate(iou_thresholds_available):\n",
    "    plt.plot(ar_values[:, i], label=f\"IoU={iou_thresh}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Recall\")\n",
    "plt.title(\"Average Recall vs. Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(images, model, confidence_threshold, allowed_labels = [3, 4, 5, 6]):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    mean=[0.485, 0.456, 0.406] \n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    # Preprocess the images\n",
    "    ims = list(image.to(device) for image in images)\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(ims)\n",
    "        #good = torch.argwhere(scores > confidence_threshold)\n",
    "        print(predictions)\n",
    "\n",
    "    for image, prediction in zip(images, predictions):\n",
    "        # Convert the image tensor to a PIL Image\n",
    "        #image_pil = transforms.ToPILImage()(image)\n",
    "\n",
    "        # Unnormalize the image\n",
    "        image_pil = transforms.ToPILImage()(image)\n",
    "\n",
    "\n",
    "        # Get the predicted bounding boxes, labels, and scores\n",
    "        boxes = prediction['boxes'].cpu().numpy()\n",
    "        labels = prediction['labels'].cpu().numpy()\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "\n",
    "        # Visualize the image and predicted bounding boxes\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.imshow(image_pil)\n",
    "\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if label in allowed_labels and score > confidence_threshold:\n",
    "                x_min, y_min, x_max, y_max = box\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                class_name = list(class_mapping.keys())[list(class_mapping.values()).index(label)]\n",
    "                rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x_min, y_min, f\"{class_name}\", color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "        plt.show()\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = next(iter(test_dataloader))\n",
    "\n",
    "visualize_prediction(samples[0], model, 0.3, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
