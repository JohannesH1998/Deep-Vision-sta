{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Importiere das Modul 'os' für die Arbeit mit dem Betriebssystem\n",
    "import json  # Importiere das Modul 'json' für die Arbeit mit JSON-Daten\n",
    "import torch  # Importiere das PyTorch-Framework\n",
    "import torchvision.transforms as transforms  # Importiere Transformationen für Bilder in PyTorch\n",
    "import torch.nn as nn  # Importiere die Klasse 'nn.Module' aus dem PyTorch-Modul 'torch.nn'\n",
    "import matplotlib.pyplot as plt  # Importiere 'matplotlib.pyplot' zum Plotten von Diagrammen und Bildern\n",
    "import matplotlib.patches as patches  # Importiere 'matplotlib.patches' zum Zeichnen von Rechtecken und Formen\n",
    "import numpy as np  # Importiere das Modul 'numpy' für numerische Berechnungen\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # Importiere Funktionen zum Aufteilen von Daten in Trainings- und Testdaten\n",
    "from torch.utils.data import Dataset  # Importiere die Klasse 'Dataset' aus 'torch.utils.data' für die Erstellung eines benutzerdefinierten Datasets\n",
    "from torchvision.transforms import ToTensor  # Importiere die Transformation 'ToTensor' aus 'torchvision.transforms' für die Konvertierung von Bildern in Tensoren\n",
    "\n",
    "from torchvision.models.detection import ssd  # Importiere das Modell 'ssd300_vgg16' aus 'torchvision.models.detection'\n",
    "from PIL import Image  # Importiere die Klasse 'Image' aus dem Modul 'PIL' für die Arbeit mit Bildern\n",
    "\n",
    "from tqdm import tqdm  # Importiere die Klasse 'tqdm' für die Fortschrittsanzeige\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir  # Das Stammverzeichnis des Datasets\n",
    "        self.images_dir = os.path.join(root_dir, \"images\")  # Verzeichnis mit den Bildern\n",
    "        self.annotations_dir = os.path.join(root_dir, \"annotations\")  # Verzeichnis mit den Annotationen\n",
    "        \n",
    "    def load_dataset(self):\n",
    "        dataset = []  # Liste für das Dataset\n",
    "        supported_image_extensions = (\".jpg\", \".jpeg\", \".png\")  # Unterstützte Bildformate\n",
    "        supported_annotation_extensions = (\".jpg.json\", \".jpeg.json\", \".png.json\")  # Unterstützte Annotationen (JSON-Dateien)\n",
    "        \n",
    "        for filename in os.listdir(self.images_dir):  # Schleife über alle Dateien im Verzeichnis mit den Bildern\n",
    "            if filename.lower().endswith(supported_image_extensions):  # Prüfe, ob die Datei eine unterstützte Bildendung hat\n",
    "                image_path = os.path.join(self.images_dir, filename)  # Pfad zur Bilddatei\n",
    "                \n",
    "                # Annotationen\n",
    "                annotation_filename = os.path.splitext(filename)[0]  # Dateiname ohne Erweiterung\n",
    "                for extension in supported_annotation_extensions:  # Suche nach einer unterstützten Annotation für das Bild\n",
    "                    annotation_file = annotation_filename + extension\n",
    "                    annotation_path = os.path.join(self.annotations_dir, annotation_file)\n",
    "                    if os.path.exists(annotation_path):\n",
    "                        break  # Breche die Schleife ab, sobald eine passende Annotation gefunden wurde\n",
    "\n",
    "                # Lade das Bild und die Annotationen\n",
    "                image, annotations = self._read_data(image_path, annotation_path)\n",
    "                dataset.append((image, annotations))  # Füge das Bild und die Annotationen zum Dataset hinzu\n",
    "\n",
    "        return dataset  # Gib das geladene Dataset zurück\n",
    "\n",
    "    def _read_data(self, image_path, annotation_path):\n",
    "        image = Image.open(image_path)  # Öffne das Bild mit PIL\n",
    "        \n",
    "        with open(annotation_path, 'r') as f:  # Öffne die Annotationen als JSON-Datei\n",
    "            annotations = json.load(f)  # Lade die Annotationen aus der JSON-Datei\n",
    "        \n",
    "        image_annotations = {  # Erstelle ein Dictionary für Bild und Annotationen\n",
    "            \"filename\": annotations[\"FileName\"],  # Dateiname des Bildes\n",
    "            \"annotations\": []  # Liste für die Annotationen des Bildes\n",
    "        }\n",
    "        \n",
    "        for annotation in annotations[\"Annotations\"]:  # Schleife über alle Annotationen\n",
    "            bbox = annotation[\"BoundingBox\"]  # Bounding-Box-Koordinaten\n",
    "            xmin, ymin, xmax, ymax = bbox  # Koordinaten der Bounding Box\n",
    "            label = annotation[\"classname\"]  # Label der Annotation\n",
    "            \n",
    "            formatted_annotation = {\n",
    "                \"bbox\": [xmin, ymin, xmax, ymax],  # Bounding-Box-Koordinaten als Liste\n",
    "                \"label\": label  # Label der Annotation\n",
    "            }\n",
    "            \n",
    "            image_annotations[\"annotations\"].append(formatted_annotation)  # Füge die formatierte Annotation zum Bild hinzu\n",
    "        \n",
    "        return image, image_annotations  # Gib das Bild und die Annotationen zurück\n",
    "    \n",
    "    \n",
    "    \n",
    "root_dir = r\"C:\\Users\\Domi\\Documents\\GitHub\\Deep-Vision-sta\\Datasets\\Face Mask Detection Dataset\\Medical mask\\Medical mask\\Medical Mask\"  # Wurzelverzeichnis des Datasets\n",
    "\n",
    "dataset_loader = DatasetLoader(root_dir)  # Erstelle einen DatasetLoader mit dem Wurzelverzeichnis\n",
    "dataset = dataset_loader.load_dataset()  # Lade das Dataset\n",
    "image, annotations = dataset[0]  # Nehme das erste Bild und die zugehörigen Annotationen\n",
    "print(len(dataset))  # Gib die Anzahl der geladenen Bilder im Dataset aus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset  # Das zugrunde liegende Dataset\n",
    "        self.transform = ToTensor()  # Transformation zum Konvertieren des Bildes in einen Tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)  # Gib die Länge des zugrunde liegenden Datasets zurück\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]  # Hole das Datenobjekt aus dem zugrunde liegenden Dataset\n",
    "        image = self.transform(data[0])  # Wende die Transformation auf das Bild an, um es in einen Tensor umzuwandeln\n",
    "        annotations = data[1]  # Behalte die Annotationen bei\n",
    "\n",
    "        return image, annotations  # Gib das Bild und die Annotationen zurück\n",
    "    \n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = []  # Liste für die Bilder\n",
    "    annotations = []  # Liste für die Annotationen\n",
    "    annotation_list = []  # Separate Liste für die Annotationen\n",
    "\n",
    "    target_size = (300, 300)  # Zielgröße für das Rescaling\n",
    "\n",
    "    rescale_transform = transforms.Resize(target_size, interpolation=Image.Resampling.BILINEAR)  # Rescaling-Transformation\n",
    "\n",
    "    for image, annotation in batch:\n",
    "        image_size = image.size()  # Größe des Bildes\n",
    "        image = transforms.ToPILImage()(image)  # Konvertiere den Tensor in eine PIL-Image-Instanz\n",
    "\n",
    "        image = rescale_transform(image)  # Wende die Rescaling-Transformation auf das Bild an\n",
    "\n",
    "        image = transforms.ToTensor()(image) images.append(image)  # Füge das Bild zur Liste der Bilder hinzu\n",
    "\n",
    "        width_ratio = target_size[0] / image_size[2]  # Verhältnis der Breiten\n",
    "        height_ratio = target_size[1] / image_size[1]  # Verhältnis der Höhen\n",
    "\n",
    "        for bbox_dict in annotation['annotations']:\n",
    "            bbox = bbox_dict['bbox']\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            x_min *= width_ratio  # Skaliere die Bounding-Box-Koordinaten entsprechend dem Verhältnis der Breiten\n",
    "            y_min *= height_ratio  # Skaliere die Bounding-Box-Koordinaten entsprechend dem Verhältnis der Höhen\n",
    "            x_max *= width_ratio  # Skaliere die Bounding-Box-Koordinaten entsprechend dem Verhältnis der Breiten\n",
    "            y_max *= height_ratio  # Skaliere die Bounding-Box-Koordinaten entsprechend dem Verhältnis der Höhen\n",
    "            bbox_dict['bbox'] = [x_min, y_min, x_max, y_max]  # Aktualisiere die Bounding-Box-Koordinaten in der Annotation\n",
    "\n",
    "        annotations.append(annotation)  # Füge die Annotation zur Liste der Annotationen hinzu\n",
    "\n",
    "    images = torch.stack(images)  # Staple die Bilder zu einem Tensor\n",
    "\n",
    "    return images, annotations  # Gib die Bilder und Annotationen zurück\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = ssd.ssd300_vgg16(num_classes=20)  # Initialisiere das Modell 'ssd300_vgg16' mit 20 Klassen\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)  # Teile das Dataset in Trainings- und Testdaten auf\n",
    "\n",
    "train_dataset = MyCustomDataset(train_data)  # Erstelle ein benutzerdefiniertes Trainingsdataset\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)  # Erstelle den Trainingsdataloader mit Stapelung\n",
    "#train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "\n",
    "test_dataset = MyCustomDataset(test_data)  # Erstelle ein benutzerdefiniertes Testdataset\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)  # Erstelle den Testdataloader mit Stapelung\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # Definiere den Optimizer mit SGD und Lernrate 0.001 und Momentum 0.9\n",
    "criterion = nn.CrossEntropyLoss()  # Definiere die Loss-Funktion als Cross-Entropy-Loss\n",
    "\n",
    "\n",
    "\n",
    "def visualize_sample(dataloader, x):\n",
    "    images, annotations = next(iter(dataloader))  # Hole das x-te Element aus dem Dataloader\n",
    "    image = images[x]  # Wähle das x-te Bild\n",
    "    boxes = annotations[x]['annotations']  # Hole die Annotationen für das x-te Bild\n",
    "\n",
    "    fig, ax = plt.subplots(1)  # Erstelle eine neue Figur und Achse\n",
    "\n",
    "    ax.imshow(image.permute(1, 2, 0))  # Zeige das Bild in der Achse an\n",
    "\n",
    "    for box in boxes:  # Iteriere über die Bounding Boxes und zeichne Rechtecke in der Achse\n",
    "        x_min, y_min, x_max, y_max = box['bbox']\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()  # Zeige die visualisierten Bounding Boxes an\n",
    "\n",
    "visualize_sample(train_dataloader, 4)  # Visualisiere das 4. Beispiel im Trainingsdataloader\n",
    "\n",
    "\n",
    "\n",
    "def draw_image_with_boxes(image, target):\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()  # Konvertiere das Bild in ein NumPy-Array\n",
    "\n",
    "    boxes = target[\"boxes\"]  # Bounding-Box-Koordinaten\n",
    "    labels = target[\"labels\"]  # Labels\n",
    "    boxes = boxes.cpu().numpy()  # Konvertiere die Bounding-Box-Koordinaten in ein NumPy-Array\n",
    "    labels = labels.cpu().numpy()  # Konvertiere die Labels in ein NumPy-Array\n",
    "        \n",
    "    fig, ax = plt.subplots(1)  # Erstelle eine neue Figur und Achse\n",
    "    \n",
    "    ax.imshow(image)  # Zeige das Bild in der Achse an\n",
    "    \n",
    "    for box, label in zip(boxes, labels):  # Iteriere über die Bounding Boxes und zeichne Rechtecke in der Achse\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min, f\"Label: {label}\", color='r', fontsize=8, bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "    plt.show()  # Zeige die Achse\n",
    "\n",
    "# Trainingsschleife\n",
    "num_epochs = 10  # Anzahl der Epochen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Verwende die GPU, falls verfügbar, ansonsten die CPU\n",
    "model.to(device)  # Verschiebe das Modell auf das entsprechende Gerät (CPU oder GPU)\n",
    "model.train()  # Setze das Modell in den Trainingsmodus\n",
    "\n",
    "for epoch in range(num_epochs):  # Schleife über die Epochen\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))  # Fortschrittsanzeige über den Trainingsdataloader\n",
    "    total_loss = 0.0  # Gesamtverlust pro Epoche\n",
    "    \n",
    "    for images, annotations in pbar:  # Schleife über die Batches im Trainingsdataloader\n",
    "        images = images.to(device)  # Verschiebe die Bilder auf das entsprechende Gerät (CPU oder GPU)\n",
    "\n",
    "        targets = []  # Liste für die Annotationen\n",
    "\n",
    "        class_mapping = {  # Mapping der Klassenlabels\n",
    "            \"hijab_niqab\": 0,\n",
    "            \"mask_colorful\": 1,\n",
    "            \"mask_surgical\": 2,\n",
    "            \"face_no_mask\": 3,\n",
    "            \"face_with_mask_incorrect\": 4,\n",
    "            \"face_with_mask\": 5,\n",
    "            \"face_other_covering\": 6,\n",
    "            \"scarf_bandana\": 7,\n",
    "            \"balaclava_ski_mask\": 8,\n",
    "            \"face_shield\": 9,\n",
    "            \"other\": 10,\n",
    "            \"gas_mask\": 11,\n",
    "            \"turban\": 12,\n",
    "            \"helmet\": 13,\n",
    "            \"sunglasses\": 14,\n",
    "            \"eyeglasses\": 15,\n",
    "            \"hair_net\": 16,\n",
    "            \"hat\": 17,\n",
    "            \"goggles\": 18,\n",
    "            \"hood\": 19\n",
    "        }  # Mapping der Klassen zu numerischen Werten\n",
    "\n",
    "        for annotation in annotations:  # Schleife über die Annotationen\n",
    "            boxes = annotation[\"annotations\"]  # Bounding-Box-Koordinaten\n",
    "            labels = [box[\"label\"] for box in boxes]  # Labels der Bounding Boxes\n",
    "            bboxes = [box[\"bbox\"] for box in boxes]  # Bounding-Box-Koordinaten\n",
    "\n",
    "            labels = [class_mapping[label] for label in labels]  # Konvertiere die Labels in numerische Werte\n",
    "\n",
    "            target = {\n",
    "                \"boxes\": torch.tensor(bboxes, dtype=torch.float32).to(device),  # Konvertiere die Bounding-Box-Koordinaten in Tensoren und verschiebe sie auf das entsprechende Gerät\n",
    "                \"labels\": torch.tensor(labels).to(device)  # Konvertiere die Labels in Tensoren und verschiebe sie auf das entsprechende Gerät\n",
    "            }\n",
    "            targets.append(target)  # Füge die Annotationen zur Liste der Annotationen hinzu\n",
    "\n",
    "        optimizer.zero_grad()  # Setze die Gradienten zurück\n",
    "\n",
    "        loss_dict = model(images, targets)  # Berechne den Verlust\n",
    "        losses = sum(loss for loss in loss_dict.values())  # Summiere die Verluste\n",
    "        losses.backward()  # Backpropagation\n",
    "        optimizer.step()  # Aktualisiere die Gewichte des Modells\n",
    "\n",
    "        total_loss += losses.item()  # Addiere den Verlust zum Gesamtverlust\n",
    "        average_loss = total_loss / (pbar.n + 1)  # Berechne den durchschnittlichen Verlust pro Batch\n",
    "\n",
    "        pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\")  # Aktualisiere die Fortschrittsanzeige\n",
    "\n",
    "    pbar.close()  # Schließe die Fortschrittsanzeige\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
